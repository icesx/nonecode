###1 虚数：i^2=-1
###2 实数:
###3 无理数:无线不循环
###4 有理数：
###5 范数：
1. 范数(norm)，是具有“长度”概念的函数			
2. 也叫L0范数：
  L0范数是指向量中非0的元素的个数
3. 具有长度的函数，每个向量的欧式范数就是向量的长度
4. L1范数：是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）
5. L2范数：指向量各元素的平方和然后求平方根,向量与原点的欧式距离
6. L2范数等于欧几里德范数：在n维欧几里德空间，向量X={x1,x2,x3}的最符合直觉的长度由以下公式给出||X||=元素的平方和然后求平方根
###6 矩阵：
1. 矩阵就是映射
2. m*n矩阵表示从n维空间到m维空间的样式，该映射是基于基底的。
###7 行列式：
	将一个 n*n的矩阵A映射到一个标量，记作det(A)或 |A|
###8 线性变换：
1. 就是把一根线（向量）变成另一根线（向量），线的变化的地方大多是方向和长度一块变
2. 线性变换（linear transformation）是线性空间V到其自身的线性映射
###9 线性映射
1. 是从一个向量空间V到另一个向量空间W的映射且保持加法运算和数量乘法运算
2. 加法和数量乘法
### 10 转置
### 11 共轭
### 12 内积空间
	内积空间是数学中的线性代数里的基本概念，是增添了一个额外的结构的向量空间。这个额外的结构叫做内积或标量积。内积将一对向量与一个标量连接起来，允许我们严格地谈论向量的“夹角”和“长度”，并进一步谈论向量的正交性。
	内积空间由欧几里得空间抽象而来（内积是点积的抽象），这是泛函分析讨论的课题。
### 13 欧式空间
	设V是实数域R上的线性空间（或称为向量空间），若V上定义着正定对称双线性型g（g称为内积），则V称为（对于g的）内积空间或欧几里德空间（有时仅当V是有限维时，才称为欧几里德空间）。[3]  具体来说，g是V上的二元实值函数，满足如下关系：
（1）g(x,y)=g(y,x)；
（2）g(x+y,z)=g(x,z)+g(y,z)；
（3）g(kx,y)=kg(x,y)；
（4）g(x,x)>=0，而且g(x,x)=0当且仅当x=0时成立。
这里x,y,z是V中任意向量，k是任意实数。
###14 欧式距离
1. 高维失真严重
2. 
### 15 迪卡尔坐标系
### 16 象限
### 17 矩阵乘法
### 18 余子式
### 19 特征向量：
1. 特征向量是在矩阵变换下不改变方向的向量，可能成为负方向，但是角度不发生变化
	
### 20 特征值

### 21 方差：
1. 一个随机变量的方差描述的是它的离散程度，也就是该变量离其期望值的距离
2. 说白了，就是将各个误差将之平方（而非取绝对值），使之肯定为正数，相加之后再除以总数，透过这样的方式来算出各个数据分布、零散（相对中心点）的程度
### 22 标准差：
1. 方差的算术平方根称为该随机变量的标准差
### 23 期望值：
1. 一个离散性随机变量的期望值（或数学期望、或均值，亦简称期望，物理学中称为期待值）是试验中每次可能的结果乘以其结果概率的总和
2. 换句话说，期望值像是随机试验在同样的机会下重复多次，所有那些可能状态平均的结果，便基本上等同“期望值”所期望的数
3. 掷一枚公平的六面骰子，其每次“点数”的期望值是3.5,不过如上所说明的，3.5虽是“点数”的期望值，但却不属于可能结果中的任一个，没有可能掷出此点数
`1*1/6+2*1/6+....=(1+2+3+4+5+6)/6`
4. 离散值期望值使用求和符号 E(x)=$sumf(xk)Pk$
5. 连续型期望值采用积分符号 E(x)=Sxf(x)dx
$\sum{k=1}^nkx
	
$a_i$ 

$int_a^b
### 24 对角化
### 25 向量：
1. 指一个同时具有大小和方向（比如：东、南、西、北）的几何对象
2. 向量的大小也叫做范数或者模长
### 26 列向量：
	在线性代数中，列向量（Column vector）是一m × 1的矩阵，即矩阵由一个包含 m个元素的列组成
### 27 内积：
1. 内积将一对向量与一个标量连接起来，允许我们严格地谈论向量的“夹角”和“长度”，并进一步谈论向量的正交性
2. 两个向量a = [a1, a2,…, an]和b = [b1, b2,…, bn]的点积定义为：a·b=a1b1+a2b2+……+anbn。
3. 数量积
4. 几何意义：向量a在向量b方向上的投影与向量b的模的乘积
### 28 向量积
1. 向量积  
  	数学中又称外积、叉积，物理中称矢积、叉乘，是一种在向量空间中向量的二元运算。
2. 与点积不同，它的运算结果是一个向量而不是一个标量。并且两个向量的叉积与这两个向量和垂直
3. 几何意义：c是垂直a、b所在平面。C模长是以|b|·sinθ为高、|a|为底的平行四边形的面积
4. a×b=|a||b|sinO n,n为a，b构成的平面的垂直单位向量
### 29 向量空间：
1. V + V → V，F × V → V，V中的元素成为向量，F中的元素成为标量
2. 一般来说，当齐次线性方程组中未知数个数大于方程的个数时，方程组有无限多组解，并且这些解组成一个向量空间。
### 30 标量：
	又称純量，是只有大小，没有方向的量，所以可以用实数表示的一个量。  
	
### 31 SVD分解：
  	奇异值分解（singular value decomposition）
### 32 对角化：
### 33 对角矩阵：
  	对角矩阵（英语：diagonal matrix）是一个主对角线之外的元素皆为0的矩阵
### 34 基：
1. 向量空间的基是它的一个特殊的子集
2. 向量空间中任意一个元素，都可以唯一地表示成基向量的线性组合
3. 给定一个向量空间V，V的一组基为可线性生成V的线性无关子集
4. 一般用e来表示
### 35 线性无关【线性独立】
1. 在线性代数里，向量空间的一组元素中，若没有向量可用有限个其他向量的线性组合所表示，则称为线性无关或线性独立，反之称为线性相关。例如在三维欧几里得空间R3的三个向量(1, 0, 0)，(0, 1, 0)和(0, 0, 1)线性无关。但(2, −1, 1)，(1, 0, 1)和(3, −1, 2)线性相关，因为第三个是前两个的和。
2. 【就是一组不能互相转换的的向量】
### 36 矩阵的秩：
1. 在线性代数中，一个矩阵A的列秩是A的线性独立的纵列的极大数目。
### 37 欧氏距离
1. 、两个点 A = (a[1]，a[2]，…，a[n]) 和 B = (b[1]，b[2]，…，b[n]) 之间的距离 ρ(A，B) 定义为下面的公式：
	`ρ(A，B) =√ [ ∑( a[i] - b[i] )^2 ] (i = 1，2，…，n)`
	
### 38 过拟合
1. 模型训练时候的误差很小，但在测试的时候误差很大，也就是我们的模型复杂到可以拟合到我们的所有训练样本了，但在实际预测新的样本的时候，糟糕的一塌糊涂
2. 通俗的讲就是应试能力很强，实际应用能力很差。擅长背诵知识，却不懂得灵活利用知识
### 39 皮尔森相关系数（Pearson correlation coefficient）
1. 也称皮尔森积矩相关系数(Pearson product-moment correlation coefficient) ，是一种线性相关系数。皮尔森相关系数是用来反映两个变量线性相关程度的统计量

### 40 指数函数
1. 是形式为 b^x的数学函数，其中  b是底数（或称基数，base），而  x是指数（index / exponent）。
2. 现今指数函数通常特指以 e为底数的指数函数（即 e^x ）,也可写作 exp(x)
4. 指数函数，形如f(x)=a^x的函数，或专指 f(x)=e^x
5. 数学运算中幂运算的上标，即 a的b次方（a^b），其中的 b即为指数
6. 3^4=81:以3为底，81的对数为4
### 41 e
1. 作为数学常数，是自然对数函数的底数。有时被称为欧拉数（Euler's number）

### 42 对数
1. 底数（英语：radix 或 base，通常简称为底），又称基数；指的是指数 b^n 中的 b，或是对数 logb 中的 b。这里的 n 称为幂，b^n 代表“以 b 为底数的 n 次幂”；
2. 而 logb 称为“以 b 为底数的对数”
3. 通常 b 与 n 是非零的实数或复数。

###43 最小二乘法（又称最小平方法）
1. 是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配
2. 最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小
3. 最小二乘法采用的手段是尽量使得等号两边的方差最小，如果是二元函数，则使采样点的方差和最小等于0，然后求解方程即可
### 44 导数
1. 导数（英语：Derivative）是微积分学中重要的基础概念
2. 一个函数在某一点的导数描述了这个函数在这一点附近的变化率
3. 导数的本质是通过极限的概念对函数进行局部的线性逼近
4. 当函数 f的自变量在一点 x0 上产生一个增量 h时，函数输出值的增量与自变量增量 h的比值在 h趋于0时的极限如果存在，则称函数河道，此极限即为 f在 x0处的导数

### 45 偏导
1. 一个多变量的函数的偏导数是它关于其中一个变量的导数，而保持其他变量恒定（相对于全导数，在其中所有变量都允许变化）
2. 设函数 z = f ( x , y ) 在点 ( x 0 , y 0 ) 具有偏导数,且在点 ( x 0 , y 0 ) 处有极值,则它在该点的偏导数必然为零:f x ( x 0 , y 0 ) = 0 ,f y ( x 0 , y 0 ) = 0

### 46 梯度下降法
1. （英语：Gradient descent）是一个一阶最优化算法，通常也称为最速下降法
2. 在单变量的实值函数的情况，梯度只是导数，或者，对于一个线性函数，也就是线的斜率
3. 梯度一词有时用于斜度，也就是一个曲面沿着给定方向的倾斜程度。可以通过取向量梯度和所研究的方向的内积来得到斜度。梯度的数值有时也被称为梯度。
### 47 损失函数
1. 损失函数是指一种将一个事件（在一个样本空间中的一个元素）映射到一个表达与其事件相关的经济成本或机会成本的实数上的一种函数
2. 损失函数参数的真值为（θ），决策的结果为d ，两者的不一致会带来一定的损失，这种损失是一个随机变量，用L(θ,d)表示。常见的损失函数有L(θ,d) = c(θ − d)^2(称为二次损失函数）
### 48 酉矩阵
1. 矩阵U*U^T=I,I为单位矩阵，U^T为U的共轭转置矩阵，则U为酉矩阵
2. U的行向量和列向量为标准正交基
### 49 埃尔米特矩阵
1. 艾尔米特证明了如果矩阵等于其复共轭转置，则特征根为实数。这种矩阵后来被称为埃尔米特矩阵
### 50 非奇异矩阵
1. 对一个 n阶方阵 A，如果存在一个 n,阶方阵 B，使  AB=BA=I AB=BA=I,I未单位矩阵，则A是可逆矩阵
2. 也称为非奇异矩阵
### 51 正交
1. 是线性代数的概念，是垂直这一直观概念的推广。
2. 作为一个形容词，只有在一个确定的内积空间中才有意义
3. 若内积空间中两向量的内积为0，则称它们是正交的

### 52 sin
1. 正弦，对边/斜边
### 53 cos
2. 余弦，邻边/斜边
### 54 tan
3. 正切，对边/邻边
### 55 cot
4. 余切，邻边/对边
### 56 点积
1. 是两个向量上的函数并返回一个标量的二元运算，它的结果是欧几里得空间的标准内积
2. 两个向量a = [a1, a2,…, an]和b = [b1, b2,…, bn]的点积定义为：
`a·b=a1b1+a2b2+……+anbn`
### 57 奇异值分解
1. 奇异值往往对应着矩阵中隐含的重要信息，且重要性和奇异值大小正相关。每个矩阵A都可以表示为一系列秩为1的“小矩阵”之和，而奇异值则衡量了这些“小矩阵”对于A的权重
### 58 ln	
1. 自然对数,(Natural logarithm）是以e为底数的对数函数，标记作ln(x)或loge(x)，其反函数是指数函数ex
2. 
### 59 微分
1. 计算导数的方法就叫微分学
2. 微分学主要研究的是在函数自变量变化时如何确定函数值的瞬时变化率（导数或微商
3. 
### 60 积分
1. 积分是微分的逆运算，即从导数推算出原函数，又分为定积分与不定积分
2. 不定积分是导数的逆运算，即反导数。当  f是 F的导数时，  F是  f的不定积分
3. 
### 61 皮尔逊相关系数
1. 两个变量之间的皮尔逊相关系数定义为两个变量之间的协方差和标准差的商：

### 62 row picture
	平面图
### 63 column picture
	向量图

### 64 回归
1. 指研究一组随机变量(Y1 ，Y2 ，…，Yi)和另一组(X1，X2，…，Xk)变量之间关系的统计分析方法，又称多重回归分析。通常Y1，Y2，…，Yi是因变量，X1、X2，…，Xk是自变量。
2. 回归在数学上来说是给定一个点集，能够用一条曲线去拟合之，如果这个曲线是一条直线，那就被称为线性回归，如果曲线是一条二次曲线，就被称为二次回归
### 65 贝叶斯
	
0. 贝叶斯公式
>P(A|B)=P(B|A)*P(A)/P(B)
>推倒：P(A交B)=P(B)*P(A|B)=P(A)*P(B|A),P(A|B)=P(A)*P(A|B)/P(B)
1. 现分别有 A、B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，现已知从这两个容器里任意抽出了一个球，且是红球，问这个红球是来自容器 A 的概率是多少?
>假设已经抽出红球为事件 B，选中容器 A 为事件 A，则有：P(B) = 8/20，P(A) = 1/2，P(B|A) = 7/10，按照公式，则有： 
`P(A|B) = (7/10)*(1/2) / (8/20) = 0.875`
2. 全概率公式
P(A)=求和【P(Bi)P(A|Bi)】
### 66 协方差矩阵
1. 协方差就是这样一种用来度量两个随机变量关系的统计量；从协方差可以引出“相关系数”的定义。协方差矩阵是一个对称的矩阵，而且对角线是各个维度上的方差。协方差矩阵计算的是不同维度之间的协方差，而不是不同样本之间的。
2. 协方差矩阵是计算不同维度之间的协方差

### 67 转移概率
1. 是马尔可夫链中的重要概念，若马氏链分为m个状态组成，历史资料转化为由这m个状态所组成的序列。从任意一个状态出发，经过任意一次转移，必然出现状态1、2、……，m中的一个，这种状态之间的转移称为转移概率。

###68 数据的标准化（normalization）
1. 是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。

2. 其中最典型的就是数据的归一化处理，即将数据统一映射到[0,1]区间上，常见的数据归一化的方法有
1. min-max标准化(Min-max normalization)
2. 通过以10为底的log函数转换的方法同样可以实现归一下，具体方法如下：
3. 用反正切函数也可以实现数据的归一化
### 69 K-means
1. K值确定
	- 轮廓系数(Silhouette Coefficient)：在实际应用中，由于Kmean一般作为数据预处理，或者用于辅助分聚类贴标签。所以k一般不会设置很大。可以通过枚举，令k从2到一个固定值如10，在每个k值上重复运行数次kmeans(避免局部最优解)，并计算当前k的平均轮廓系数，最后选取轮廓系数最大的值对应的k作为最终的集群数目。  
  	- Calinski-Harabasz准则：其中SSB是类间方差， ，m为所有点的中心点,mi为某类的中心点；SSW是类内方差，；(N-k)/(k-1)是复杂度；比率越大，数据分离度越大.

###问题
	朴素贝叶斯和隐含马尔科夫的区别
	逻辑回归和朴素贝叶斯

###隐含狄利克雷分布


###正则化


###曲线回归(curvilinear regression)或非线性回归(non-linear regression)
1. 两个变数间呈现曲线关系的回归。
2. 曲线回归分析或非线性回归分析：以最小二乘法分析曲线关系资料在数量变化上的特征和规律的方法。

###高斯分布
1. 正态曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，因此人们又经常称之为钟形曲线。
2. 若随机变量X服从一个数学期望为μ、方差为σ^2的正态分布，记为N(μ，σ^2)。其概率密度函数为正态分布的期望值μ决定了其位置，其标准差σ决定了分布的幅度。当μ = 0,σ = 1时的正态分布是标准正态分布

###KNN
1. 

###最大似然估计
1. 是一种统计方法，它用来求一个样本集的相关概率密度函数的参数。这个方法最早是遗传学家以及统计学家罗纳德·费雪爵士在1912年至1922年间开始使用的。
2. 

###维特比算法
1. 它用于寻找最有可能产生观测事件序列的维特比路径——隐含状态序列，特别是在马尔可夫信息源上下文和隐马尔可夫模型中
2. 找隐含状态序列
3. 几个主要的参数
>states:可能的隐含状态值
>>states = ('Rainy', 'Sunny')

>observations:显性的观察值
>>observations = ('walk', 'shop', 'clean')	

>start_probability:初始的隐状态的发生概率
>>start_probability = {'Rainy': 0.6, 'Sunny': 0.4}

>transition_probability:隐状态的变化概率，是一个统计概率，相对固定【下雨到天晴的概率等】
>>transition_probability = {
　　　'Rainy' : {'Rainy': 0.7, 'Sunny': 0.3},
　　　'Sunny' : {'Rainy': 0.4, 'Sunny': 0.6},
　　　}

>emission_probability:隐状态下的发生观察值的概率
>>emission_probability = {
　　　'Rainy' : {'walk': 0.1, 'shop': 0.4, 'clean': 0.5},
　　　'Sunny' : {'walk': 0.6, 'shop': 0.3, 'clean': 0.1},
　　　}
###DTW
1. DTW算法又叫动态时间规整（ Dynamic Time Warping），是一个比较简单的dp算法。常用于不等长的离散的路径点的匹配问题，在孤立词语音识别、手势识别、数据挖掘和信息检索等领域有着很不错的表现
2. [i] 通过M维和N维向量比较，创建M×N维矩阵，矩阵每个点代表，向量的分量的距离。从0×0开始计算相邻的距离大小，取最小值，一直迭代到M×N点。

###熵
1. 自信息
	有这样的特性：如果事件 c 中包含了两个相互独立的事件 a 与事件 b，当事件 c 发生时,它所包含的自信息量，等于事件 a 的自信息量加上事件 b。一系列事件的自信息加总之后，平均的自信息值就是信息熵。
2. 单位，例如 bit、nat或是hart，使用哪个单位取决于在计算中使用的对数的底。

### 风险函数
1. L(Y-f(x))
### 经验风险
1. 对所有的训练样本都求一次损失函数，在累加求评价。
2. 表示决策函数对训练数据集里的预测能力
### 期望风险
1. 对所有样本【未知样本和已知的训练样本】的预测能力，是全局概念

### 最大似然估计
1. 是利用已知的样本的结果，在使用某个模型的基础上，反推最有可能导致这样结果的模型参数值。
2. 求极大似然函数估计值的一般步骤：
（1）由总体分布导出样本的联合概率密度函数；
（2）把样本联合概率密度函数中自变量看成已知常数，而把参数θ看作自变量，得到似然函数L(θ)；
（3）求似然函数的最大值点（常转化为求对数似然函数的最大值点）；
（4）在最大值点的表达式中，用样本值代入即得到参数的极大似然估计值。
### 激活函数
	
### 决策函数
	模型函数，用于对提供的数据进行计算获得结果的函数
### 先验概率
	是指根据以往经验和分析得到的概率，如全概率公式，它往往作为"由因求果"问题中的"因"出现的概率
### 后验概率
	后验概率是指在得到“结果”的信息后重新修正的概率，是“执果寻因”问题中的"果"
	后验概率的计算，要使用贝叶斯公式，而且在利用样本资料计算逻辑概率时，还要使用理论概率分布，需要更多的数理统计知识
	在贝叶斯统计中，一个随机事件或者一个不确定事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。
### 解析解(Analytical solution) 
	就是根据严格的公式推导，给出任意的自变量就可以求出其因变量，也就是问题的解，然后可以利用这些公式计算相应的问题。解析解也被称为封闭解(Closed-form solution)
### 数值解(Numerical solution) 
	是采用某种计算方法，如有限元法， 数值逼近法，插值法等得到的解。别人只能利用数值计算的结果，而不能随意给出自变量并求出计算值

###召回率、准确率、精确率
	假设我们手上有60个正样本，40个负样本，我们要找出所有的正样本，系统查找出50个，其中只有40个是真正的正样本，计算上述各指标。
	TP: 将正类预测为正类数  40
	FN: 将正类预测为负类数  20
	FP: 将负类预测为正类数  10
	TN: 将负类预测为负类数  30
	准确率(accuracy) = 预测对的/所有 = (TP+TN)/(TP+FN+FP+TN) = 70%
	精确率(precision) = TP/(TP+FP) = 80%
	召回率(recall) = TP/(TP+FN) = 2/3